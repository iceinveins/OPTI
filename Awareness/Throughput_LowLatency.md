# <font color="3d8c95">吞吐量与低延迟</font>
吞吐：单位时间内做的有用功  
响应：低延迟  
吞吐追求的整个系统CPU做有用功，响应追求的是某个特定任务的延迟低；  
1GHZ的CPU切换线程保存恢复现场约几个微妙级别，看似消耗不了太多时间，但是由于系统的局部性原理，会保存当前线程数据的缓存，切换线程会打乱局部性引起cache miss，而CPU访问cache速度远大于内存访问，这样综合看来上下文切换花销还是很大的。无用功占用较多CPU；  
<font color="fed3a8">追求吞吐量和低延迟，这两个目标是矛盾的</font>

1. 要控制和降低延迟，首先要能<font color="fed3a8">准确测量延迟</font>，因此需要比较准的钟，每个机房配几个带GPS和/或原子钟primary standard的NTP服务器是少不了的。而且就算用了NTP，同一机房两台机器的时间也会有毫秒级的差异，计算延迟的时候，两台机器的时间戳不能直接相减，因为不在同一时钟域。解决办法是设法补偿这个时差。另外，<font color="fed3a8">不仅要测量平均延迟，更重要的是要测量并控制长尾延迟</font>，即99百分位数或99.9百分位数的延迟，就算是sell side，系统偶尔慢一下被speculator利用了也是要亏钱的。
2. 普通的C++服务程序，内部延迟（从进程收到消息到进程发出消息）做到几百微秒（即亚毫秒级）是不需要特殊的努力的。没什么忌讳，该怎么写就怎么写，不犯低级错误就行。我很纳闷国内流传的写 C++ 服务程序时的那些“讲究”是怎么来的（而且还不是 latency critical 的服务程序）。<font color="fed3a8">如果瓶颈在CPU，那么最有效的优化方式是“强度消减”，即不在于怎么做得快，而在于怎么做得少。</font>哪些可以不用做，哪些可以不提前做，哪些做一次就可以缓存起来用一阵子，这些都是值得考虑的。
3. 网络延迟分传输延迟和惯性延迟，通常局域网内以后者为主，广域网以前者为主。前者是传送1字节消息的基本延迟，大致跟距离成正比，千兆局域网单程是近百微秒，伦敦到纽约是几十毫秒。这个延迟受物理定律限制，优化办法是买更好的网络设备和租更短的线路（或者想办法把光速调大，据说 Jeff Dean 干过）。惯性延迟跟消息大小成正比，跟网络带宽成反比，千兆网TCP有效带宽按115MB/s估算，那么发送1150字节的消息从第1个字节离开本机网卡到第1150个字节离开本机网卡至少需要 10us，这是无法降低的，因此必要的话可以减小消息长度。举例来说，要发10k的消息，先花20us CPU时间，压缩到3k，接收端再花10us解压缩，一共“60us+传输延迟”，这比直接发送10k消息花“100us+传输延迟”要快一点点。（广域网是否也适用这个办法取决于带宽和延迟的大小，不难估算的。）
4. <font color="fed3a8">延迟和吞吐量是矛盾的</font>，通常吞吐量上去了延迟也会跟着飚上去，因此控制负载是控制延迟的重要手段。延迟跟吞吐量的关系通常是个U型曲线，吞吐量接近0的时候延迟反而比较高，因为系统比较“冷”；吞吐量上去一些，平均延迟会降到正常水平，这时系统是“温”的；吞吐量再上去一些，延迟缓慢上升，系统是“热”的；吞吐量过了某个临界点，延迟开始飙升，系统是“烫”的，还可能“冒烟”。因此要做的是把吞吐量控制在“温”和“热”的范围，不要“烫”，也不要太冷。系统启动之后要“<font color="fed3a8">预热</font>”。
5. <font color="fed3a8">延迟和资源使用率是矛盾的</font>，做高吞吐的服务程序，恨不得把CPU和IO都跑满，资源都用完。而低延迟的服务程序的资源占用率通常低得可怜，让人认为闲着没干什么事，可以再“加码”，要抵住这种压力。就算系统到了前面说的“发烫”的程度，其资源使用率也远没有到 100%。实际上平时资源使用率低是为了准备应付突发请求，请求或消息一来就可以立刻得到处理，尽量少排队，“排队”就意味着等待，等待就意味着长延迟。<font color="fed3a8">消除等待是最直接有效的降低延迟的办法，靠的就是富裕的容量。</font>有时候队列的长度也可以作为系统的性能指标，而不仅仅是CPU使用率和网络带宽使用率。另外，队列也可能是隐式的，比如操作系统和网络设备的网络输入输出 buffer 也算是队列。
6. <font color="fed3a8">延迟和可靠传输也是矛盾的</font>，TCP做到可靠传输的办法是超时重传，一旦发生重传，几百毫秒的延迟就搭进去了，因此保持网络随时畅通，避免拥塞也是控制延迟的必要手段。要注意不要让batch job抢serving job的带宽，比方说把服务器上的日志文件拷到备份存储，这件事不要在繁忙交易时段做。QoS也是办法；或者布两套网，每台机器两个网口，两个IP。
7. 最后，设法<font color="fed3a8">保证关键服务进程的资源充裕，避免侵占</font>（主要是CPU和网络带宽）。比如把服务器的日志文件拷到别的机器会占用网络带宽，一个办法是慢速拷贝，写个程序，故意降低拷贝速度，每50毫秒拷贝50kB，这样用时间换带宽。还可以先压缩再拷贝，比如gzip压缩100MB的服务器日志文件需要1秒，在生产服务器上会短期占满1个core的CPU资源，可能造成延迟波动。可以考虑写个慢速压缩的程序，每100毫秒压缩100kB，花一分半钟压缩完100MB数据，分散了CPU资源使用，减少对延迟的影响。千万不要为了加快压缩速度，采用多线程并发的办法，这就喧宾夺主了。


## <font color="dc843f">交易延迟分类</font>
延迟是计算机系统接收到一个事件刺激，到产生响应之间的时间间隔。对于券商而言，事件刺激可以是客户端输入订单，可以接收到市场行情数据发布，可以是接收到订单确认返回。低延迟交易要求整个交易链条上的所有环节，都尽量缩短时间间隔。从交易系统层面看，交易延迟主要包括网络延迟、协议延迟、操作系统延迟、应用延迟等。
#### <font color="dc843f">网络延迟</font>
交易系统的下单通过网络经券商柜台到达交易所交易撮合主机，中间会经过多个网络设备，包括交换机、路由器和防火墙等，因此网络会存在延迟。
网络中存在三种类型的延迟：数据序列化延迟、传输延时、排队延迟。
- 数据序列化延迟  
数据序列化延迟是网络设备将一定量的数据位（bit）输入物理介质（通常指光纤或者铜缆）所需要的时间。
- 传输延迟  
传输延时传输延时（propagation）是数据经过序列化处理进入传输介质后，在传输介质中传输所使用的时间。光在光纤中传输速度通常是在真空中传输速度的三分之二。
- 排队延迟  
当多个数据发送端通过同一条网络链路往一个接收端发送数据包时，数据包必须在连接发送端与接收端的交换设备上进行排队（queuing）。一个数据包排队延时的大小取决于两个因素，一个是排在前面的数据包数量，一个是数据接收链路的带宽速率。
#### <font color="dc843f">协议延迟</font>
交易网络中存在多种网络传输协议，比如TCP、UDP、SPX等，由于不同协议设计的目标不同，因此其传输效率以及相应的延迟都有所区别。针对网络协议延迟，需要考虑以下特性：
- 纳吉尔算法（Nagle Algorithm）
- 数据窗口（Data Window）耗尽
- 包丢失（Packet Loss）
- 路由协议
- QOS与SPAN的使用
#### <font color="dc843f">操作系统延迟</font>
在计算机硬件与交易系统间，存在BIOS/FIRMWARE、操作系统内核、代码运行环境（包含在操作系统内），为交易系统提供各种功能函数调用，以实现代码与硬件之间的互动，操作系统内核及代码运行环境在调用过程中也存在一定的延迟。  
设备制造厂商一般建议将BIOS和Firmware更新到最新版本并进行特别设置，并且提供关于低延时方面的解决方案或最佳实践，包括提供配置清单和脚本，用于检查各层次基础软件设置是否符合低延迟系统最佳实践等。
#### <font color="dc843f">应用延迟</font>
低延迟应用一般是交易机构自行控制的，根据一般交易应用的本质特征，应用延迟分为：
- 磁盘访问  
低延时应用采用内存数据库或者固态存储来降低IO延迟。
- 进程间通讯  
低延时应用采用RDMA等技术降低进程间通讯成本。
- 数据计算  
低延时应用采用FPGA、GPU等技术加速数据计算过程。
- 数据锁等共享资源访问等待
可采用非阻塞式的数据管理系统，或者更小粒度的资源管理来降低访问等待。


## <font color="dc843f">延迟的度量</font>
由于低延迟交易系统的延迟，在每一个环节都接近次微秒级，在度量时，不仅要作为一个整体来考虑平均延时、延时抖动以及吞吐量，而且要注意度量手段和工具本身的时间精度。  
- 平均延时    
平均延迟是是消息通过从一个点到另一个点的平均时间,越低越好。  
- 延迟抖动  
延迟抖动是衡量延迟可预测性的重要指标，其代表的是测量过程中，每次实际延迟与平均延迟之间的偏差程度。
- 吞吐量  
吞吐量可以定义为单位时间处理的数据量。一般低延迟系统要求在保持低延迟的前提下，追求尽量大的吞吐量。
- 时间精度  
大部分的计时系统依赖硅晶体振荡和主板中断等硬件基础，不同的操作系统平台采用的精度控制方法会有所差别。部分低延迟网卡和交换机可以提供高精度的时间戳供延迟度量使用。
